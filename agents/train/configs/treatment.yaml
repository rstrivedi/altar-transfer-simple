# Added by RST: Treatment arm configuration for Phase 4
# Training configuration for treatment arm (with PERMITTED_COLOR observation)

# Environment configuration
env:
  permitted_color_index: 1  # 1=RED, 2=GREEN, 3=BLUE
  startup_grey_grace: 25
  episode_timesteps: 2000
  altar_coords: [5, 15]  # Altar position (treatment only)
  alpha: 0.5  # Train-time bonus for correct zaps
  beta: 0.5   # Mis-zap penalty
  c: 0.2      # Zap effort cost
  immunity_cooldown: 200

# Training hyperparameters (PPO)
training:
  total_timesteps: 5000000
  n_envs: 16
  seed: 42
  learning_rate: 0.0003
  n_steps: 256
  batch_size: 2048
  n_epochs: 10
  gamma: 0.995
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.01  # Default (overridden by head-wise coefficients)
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1

# Policy architecture
policy:
  trunk_dim: 256
  sanction_hidden_dim: 128
  ent_coef_game: 0.01      # Entropy coefficient for game head
  ent_coef_sanction: 0.02  # Entropy coefficient for sanction head (higher initially)
  recurrent: false         # Set to true for LSTM variant
  lstm_hidden_size: 256

# VecNormalize configuration
vec_normalize:
  norm_obs: false    # Don't normalize RGB observations
  norm_reward: true  # Normalize rewards for stable training
  clip_obs: 10.0
  clip_reward: 10.0
  gamma: 0.995

# Logging configuration
logging:
  wandb_project: altar-transfer
  wandb_entity: null  # Set to your W&B entity if needed
  wandb_run_name: null  # Auto-generated if null
  tensorboard_log: ./logs/tensorboard
  log_interval: 10

# Checkpointing configuration
checkpointing:
  save_freq: 50000  # Save checkpoint every 50k steps
  checkpoint_dir: ./checkpoints/treatment
  save_vec_normalize: true

# Evaluation configuration
evaluation:
  eval_freq: 100000  # Evaluate every 100k steps
  n_eval_episodes: 20
  eval_seeds: null  # Will generate if null (use fixed seeds for reproducibility)
