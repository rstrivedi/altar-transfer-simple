# Added by RST: Smoke test configuration for Phase 5 (Multi-Community)
# Quick training run for testing (small timesteps, fewer envs)

# Environment configuration
env:
  permitted_color_index: 1  # Placeholder (sampled at runtime)
  startup_grey_grace: 50  # Edited by RST: Updated to match substrate (was 25)
  episode_timesteps: 2000
  altar_coords: [5, 15]
  alpha: 5.0  # Edited by RST: Train-time bonus for correct zaps (was 0.5)
  beta: 5.0   # Edited by RST: Mis-zap penalty magnitude (was 0.5)
  c: 0.5      # Edited by RST: Zap effort cost magnitude (was 0.2)
  immunity_cooldown: 200

# Training hyperparameters (REDUCED for smoke test)
training:
  total_timesteps: 200000  # Only 200k steps for quick test
  n_envs: 6  # Fewer envs (divisible by 3 for balance check)
  seed: 42
  learning_rate: 0.0003
  n_steps: 256
  batch_size: 1536  # Smaller batch (6 envs * 256 steps)
  n_epochs: 10
  gamma: 0.995
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1

# Policy architecture
policy:
  trunk_dim: 256
  sanction_hidden_dim: 128
  ent_coef_game: 0.01
  ent_coef_sanction: 0.02
  recurrent: true  # Edited by RST: Enable LSTM for partial observability (was false)
  lstm_hidden_size: 256

# VecNormalize configuration
vec_normalize:
  norm_obs: false
  norm_reward: true
  clip_obs: 10.0
  clip_reward: 10.0
  gamma: 0.995

# Logging configuration
logging:
  wandb_project: altar-transfer
  wandb_entity: null
  wandb_run_name: smoke_test_multi
  tensorboard_log: ./logs/tensorboard
  log_interval: 1  # Log every step for debugging

# Checkpointing configuration
checkpointing:
  save_freq: 50000  # Save every 50k steps
  checkpoint_dir: ./checkpoints/smoke_test_multi
  save_vec_normalize: true

# Evaluation configuration
evaluation:
  eval_freq: 100000  # Eval at 100k steps
  n_eval_episodes: 5  # Per community (15 total episodes)
  eval_seeds: [42, 43, 44, 45, 46]  # Fixed seeds (will generate 15 total)
